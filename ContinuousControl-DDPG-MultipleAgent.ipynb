{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continous Control with Multiple-Agent DDPG\n",
    "---\n",
    "In this notebook, we train DDPG with Unity Reacher environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_wrapper import EnvironmentWrapper\n",
    "from ddpg_agent import Agent\n",
    "import torch\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ddpg import DDPG\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "Each observes a state with length: 33\n"
     ]
    }
   ],
   "source": [
    "env = EnvironmentWrapper()\n",
    "ag = Agent(state_size=env.state_size, action_size=env.action_size, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.59\n",
      "Episode 2\tAverage Score: 0.56\n",
      "Episode 3\tAverage Score: 0.47\n",
      "Episode 4\tAverage Score: 0.36\n",
      "Episode 5\tAverage Score: 0.31\n",
      "Episode 6\tAverage Score: 0.28\n",
      "Episode 7\tAverage Score: 0.27\n",
      "Episode 8\tAverage Score: 0.27\n",
      "Episode 9\tAverage Score: 0.30\n",
      "Episode 10\tAverage Score: 0.36\n",
      "Episode 11\tAverage Score: 0.39\n",
      "Episode 12\tAverage Score: 0.45\n",
      "Episode 13\tAverage Score: 0.48\n",
      "Episode 14\tAverage Score: 0.54\n",
      "Episode 15\tAverage Score: 0.60\n",
      "Episode 16\tAverage Score: 0.66\n",
      "Episode 17\tAverage Score: 0.72\n",
      "Episode 18\tAverage Score: 0.82\n",
      "Episode 19\tAverage Score: 0.95\n",
      "Episode 20\tAverage Score: 1.04\n",
      "Episode 21\tAverage Score: 1.15\n",
      "Episode 22\tAverage Score: 1.25\n",
      "Episode 23\tAverage Score: 1.39\n",
      "Episode 24\tAverage Score: 1.49\n",
      "Episode 25\tAverage Score: 1.62\n",
      "Episode 26\tAverage Score: 1.78\n",
      "Episode 27\tAverage Score: 1.98\n",
      "Episode 28\tAverage Score: 2.19\n",
      "Episode 29\tAverage Score: 2.39\n",
      "Episode 30\tAverage Score: 2.52\n",
      "Episode 31\tAverage Score: 2.70\n",
      "Episode 32\tAverage Score: 2.88\n",
      "Episode 33\tAverage Score: 3.06\n",
      "Episode 34\tAverage Score: 3.25\n",
      "Episode 35\tAverage Score: 3.44\n",
      "Episode 36\tAverage Score: 3.61\n",
      "Episode 37\tAverage Score: 3.79\n",
      "Episode 38\tAverage Score: 3.98\n",
      "Episode 39\tAverage Score: 4.19\n",
      "Episode 40\tAverage Score: 4.38\n",
      "Episode 41\tAverage Score: 4.53\n",
      "Episode 42\tAverage Score: 4.74\n",
      "Episode 43\tAverage Score: 4.94\n",
      "Episode 44\tAverage Score: 5.12\n",
      "Episode 45\tAverage Score: 5.30\n",
      "Episode 46\tAverage Score: 5.49\n",
      "Episode 47\tAverage Score: 5.69\n",
      "Episode 48\tAverage Score: 5.82\n",
      "Episode 49\tAverage Score: 6.05\n",
      "Episode 50\tAverage Score: 6.24\n",
      "Episode 51\tAverage Score: 6.44\n",
      "Episode 52\tAverage Score: 6.62\n",
      "Episode 53\tAverage Score: 6.83\n",
      "Episode 54\tAverage Score: 7.00\n",
      "Episode 55\tAverage Score: 7.20\n",
      "Episode 56\tAverage Score: 7.43\n"
     ]
    }
   ],
   "source": [
    "DRL = DDPG(env, ag)\n",
    "scores = DRL.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(500):\n",
    "    action = agent.act(state, add_noise=False)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
